% !TEX TS-program = pdflatex 
% !TEX encoding = UTF-8 Unicode
\documentclass[conference]{IEEEtran} % use larger type; default would be 10pt
%%% BASIC DISPLAY PACKAGES
%\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
%\usepackage{setspace}
%\doublespacing
%\usepackage{graphicx}
%\usepackage{listings}
%\usepackage{color}
%\usepackage{pdflscape}
%\usepackage{caption}
%\usepackage{pifont}
\usepackage{multirow}
\usepackage{balance}



%%% PAGE DIMENSIONS
%\usepackage{geometry} % to change the page dimensions
%\geometry{letterpaper} % or letterpaper (US) or a5paper or....
% \geometry{margins=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
% read geometry.pdf for detailed page layout information
%\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
%%% PACKAGES
%\usepackage{booktabs} % for much better looking tables
%\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage{enumerate} %for better lists
%Package and command for creating field symbol such as Q, R etc...
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{cite}
\usepackage{nopageno}
%\usepackage{MnSymbol}%allows usage of symbol of three dots in a diagonal bottom left to top right (and other dots)

\linespread{1.1}

%%%STRUCTURE BUILDER
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Propostion}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}


%%% HEADERS AND FOOTERS
%\usepackage{fancyhdr}
%\pagestyle{plain}
%\lhead{}
%\rhead{}


%%MACROS
\newcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pref}[1]{(\ref{#1})}
\newcommand{\mm}[1]{\mathbf{#1}}

%=========================yz7210==================================================%

\title{Tackling skewed data in on-line fake review detection}
\date{\today}
\author{\IEEEauthorblockN{Satheesh Joseph}
\IEEEauthorblockA{satheeshrishi@berkeley.edu}
\and
\IEEEauthorblockN{Catherine Mou}
\IEEEauthorblockA{catherine041616@berkeley.edu}
\and
\IEEEauthorblockN{Yi Zhang}
\IEEEauthorblockA{yizhang7210@berkeley.edu}
}

\begin{document}
\maketitle



\begin{abstract}
The increasing need and value of the digital text and the trustworthiness behind it inspired our imagination to handle a niche problem within the space of opinion spam analysis. Recently many researches have been conducted to increase the detection accuracy. To the best of our knowledge, an important issue that has been less studied is to tackle imbalanced data in on-line fake review detection. In this paper, we study a methodology to tackle the imbalanced data issue and the accuracy effects on different state-of-the-art models. For this research, we use Yelp review data for hotels and restaurants that is classified as needed.
\end{abstract}

\section{Introduction}
\label{intro}
On-line fake review detection is a relatively well studied subject, especially in recent years, given its outsized impact on consumers engaging in e-commerce activities online. Positive reviews bring a meaningful increase in sales volume to the products~\cite{ho2013effects}, and vice versa for negative reviews.

As a result, there has been an increase in opinion spamming activities, and detecting fake reviews has become an essential requirement for on-line marketplaces to maintain the integrity and fairness of their platform.

However, there has been one persisting challenge~\cite{stanton2019gans, Tang2020, wang2020fake, yuan2019learning} in this area of research -- the lack of substantial body of actually proven fake reviews, directly leading to significantly imbalanced datasets.

Our work aims to tackle this problem of data imbalance by borrowing ideas from Generative Adversarial Network (GAN). Our \textbf{hypothesis} is that it's possible to make up the data imbalance by generating fake reviews from a \textbf{language model} trained and/or fine-tuned on actual fake reviews. We will look to validate this approach by then training a review detection model on a balanced dataset that includes the generated fake reviews, and achieving comparable results to state-of-the-art research~\cite{Tang2020}.

Section \ref{bg} provides the background on the existing research in fake review detection. Section \ref{methods} lays out our methods of constructing the fake review generator. Section \ref{exp} discusses the experiments we run to validate the usefulness of the generated fake reviews in training a detection model. And we draw our final conclusions in section \ref{conclusion}.


\section{Background}
\label{bg}
There is no shortage of research tackling the problem of fake review detection. A recent survey~\cite{Mohawesh2021} does a great job laying out the landscape of the various techniques and data sets used for fake review detection.

According to this survey, all large datasets ($>$20k reviews) from Yelp~\cite{rayana2015collective} contain less than $15\%$ actual fake reviews. There are a few other widely used public datasets crawled from TripAdvisor, but they are of much smaller scale, with the fake review training set generated via a manual process from Amazon Mechanical Turk.

The only balanced dataset of moderate volume is crawled from Yelp by Barbado et. al.~\cite{barbado2019framework}, however it has not been widely adopted in the research community as a benchmark for detecting fake reviews.

Given this state of the related work, and inspired by Stanton et. al.~\cite{stanton2019gans} who used GAN techniques to generate behavioral features (e.g. number of reviews, percentage of positive reviews) for on-line Yelp reviewers, we believe that similar techniques can be used to generate the reviews themselves.

With sufficient representativeness, we believe the generated fake reviews can serve as additional training examples that can help with the detection model to distinguish between genuine reviews and fake ones. To paraphrase Tolstoy --  genuine reviews are all alike; every fake review is fake in its own way.



\section{Methods}
\label{methods}
The general structure of a Generative Adversarial Network consists of a \textit{generative} network that generates candidates as well as a \textit{discriminative} network that evaluates them. Here we apply this technique to generate fake review texts.

For the \textit{generative} network, we here use the well-established, pre-trained GPT-2 language model.

GPT-2 was pre-trained on web scraped industrial-scale corpus, it was trained with a batch size of 512, well-defined sentence length, volcabulary seize of 50,000., and can be used for various NLP tasks including text generation. It is a statistical tool to generate next words in sequence based on preceding words, at every stage, it will take the previously generated data as additional input when generating next output. GPT-2 has outperformed other language models when it comes to generating text based on small input contents like hotel/restaurant reviews. GPT-2 has the ability to adapt to context of text, so it can generate realistic and coherent output. 

To generate domain specific fake Yelp reviews, we took the Transformers library from huggingface and modified the language model file~\cite{gpt2guide} to fine-tune the pre-trained model using the actual fake reviews from our training set.

For the \textit{discriminative} network, we trained a simple Neural Network using ELMo embedding with 2 dense layers trained on the original data set. 

To maintain some quality of the generated fake reviews, the final set of generated fake reviews that are used for model evaluation in section \ref{exp} contain only the ones that the discriminative network made an error on. 



\section{Experiment \& Discussion}
\label{exp}

\subsection{Experimental Setup}
For our experiments, we use the publicly available data set originally obtained by \cite{mukherjee2013fake} and \cite{mukherjee2013yelp}.
This data set, containing 5858 reviews for hotels and 67019 reviews for restaurants on Yelp, is also used by a number of prior research papers for benchmarking, notably~\cite{wang2017handling} and~\cite{Tang2020}.
 A more detailed statistics of the raw data set is in Table \ref{original-data}. As mentioned above, it is indeed highly imbalanced with a much smaller number of fake reviews.

\begin{table}[h]
\normalsize
\caption{Summary of Experimental Data}
\centering
\begin{tabular}{|c|c|c|}
\hline
Subject & Hotels & Restaurants \\ \hline
Total \# Reviews & 5858 & 67019 \\ \hline
Total \# Genuine Reviews & 5078 & 58716 \\ \hline
Total \# Fake Reviews & 780 &  8303 \\ \hline
\% Fake Reviews & 13.3\% & 12.4\%  \\ 
\hline
\end{tabular}
\label{original-data}

\end{table}

To validate our hypothesis on the usefulness of the generated fake reviews, we set up a 2-stage experiment.

Firstly, we use the methods laid out in section \ref{methods} to generate fake reviews for both hotels and restaurants. Secondly, we add the generated fake reviews to the training data to obtain a balanced training set, then run a number of classification models on the mixed training set and compare it against our benchmark results.

We construct the data sets in the following ways to be comparable with the prior research papers~\cite{wang2017handling}, \cite{Tang2020}.

For a balanced test set:
\begin{itemize}
\setlength\itemsep{0em}
\item we first limit the pool of reviews to the first review per reviewer after 2012-01-01
\item we take all the fake reviews (because there are fewer)
\item we sample the same number of reviews from the genuine reviews
\item this gives us a balanced, non-duplicated test set
\end{itemize}

For a balanced training set:
\begin{itemize}
\setlength\itemsep{0em}
\item we first limit the pool of reviews to be the ones prior to 2012-01-01
\item we take all the actual fake reviews
\item we include all generated fake reviews
\item we sample the same number of reviews from the genuine reviews
\item this gives us a balanced, non-duplicated training set
\end{itemize}


\begin{table}[h]
\normalsize
\caption{Summary of Training/Test Data}
\centering
\begin{tabular}{|c|c|c|}
\hline
Subject & Hotels & Restaurants \\ \hline
Training set size & 5070 &  \\ \hline
\# Genuine Reviews & 2535  &  \\ \hline
\# Actual Fake Reviews & 561  &  \\ \hline
\# Generated Reviews & 1974 &  \\ \hline
\% Fake Reviews in training set & 50\% &  \\ \hline
Test set size & 432 &  \\ \hline
\% Fake Reviews in test set & 50\% &  \\ 
\hline
\end{tabular}
\label{trainining-data}

\end{table}

\subsection{Models}
We established a number of models all based on Neural Networks to compare our results with the benchmark.

\textbf{Model 1}: Our baseline model with GloVe embedding and 1 layer of LSTM.

\textbf{Model 2}: Our main model with GloVe embedding, 1 layer of Bidirectional LSTM, and a few more dense layers.

\textbf{Model 3}: A BERT based model.

\textbf{Model 4}: Same as Model 2 except using ELMo embedding instead of GloVe.


For each model, we'll be training on 4 different training sets for comparison:

 \textbf{Set 1}: raw, imbalanced training set
 
 \textbf{Set 2}: balanced training set by under-sampling genuine reviews to the number of fake reviews
 
 \textbf{Set 3}: balanced training set by over-sampling fake reviews with replacement to the number of genuine reviews
 
 \textbf{Set 4}: balanced training set by including generated fake reviews per Table \ref{trainining-data}

\subsection{Results}
We report the best results of each model on each training set after hyper-parameter tuning via grid search. We will also report state-of-the-art classification benchmark from~\cite{Tang2020} using generated behavior features.

\begin{table}[h]
\normalsize
\caption{Experimental Results - Hotels}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 accuracy (f1) & Set 1 & Set 2 & Set 3 & Set 4 \\ \hline
 
Model 1 & 0.51/0.35 & 0.60/0.62 & 0.57/0.57 & 0.59/0.59 \\ \hline
Model 2 & 0.56/0.63 & 0.59/0.59 & 0.57/0.62 & 0.59/0.60 \\ \hline
Model 3 & 0.5 & 0.57 & 0.53 & 0.52  \\ \hline
Model 4 &  &  & & \\ \hline
bfGAN\cite{Tang2020} & \multicolumn{4}{c|}{83.0/83.4} \\
\hline
\end{tabular}
\label{exp-hotels}
\end{table}

\begin{table}[h]
\normalsize
\caption{Experimental Results - Restaurants}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 accuracy (f1) & Set 1 & Set 2 & Set 3 & Set 4 \\ \hline
Model 1 & - & - & - & - \\ \hline
Model 2 & - & - & - & - \\ \hline
Model 3 & - & - & - & - \\ \hline
Model 4 & - & - & - & - \\ \hline
bfGAN\cite{Tang2020}& \multicolumn{4}{c|}{75.7/75.1} \\
\hline
\end{tabular}
\label{exp-restaurants}

\end{table}

\subsection{Discussion}



\section{Conclusion}
\label{conclusion}
%
%Example table
%\begin{table}[h]
%\normalsize
%\caption{Summary of Experimental Results}
%\centering
%\begin{tabular}{|c|c|c|c|c|c|}
%\hline
%System & $n$ &  $\gamma$ & Samples & mean & max \\ \hline
%\multirow{3}{*}{A} & \multirow{3}{*}{21} & 0.2 & 24236 (11\%) & 0.083 & 0.084\\
% && 0.15 & 43307 (20\%) & 0.081 & 0.081\\
%  && 0.1 & 97440 (44\%)& 0.074 & 0.074\\ \hline
%\multirow{3}{*}{B} & \multirow{3}{*}{23}  & 0.2 & 26332 (2.2\%) & 0.035 & 0.036\\
%&& 0.15 & 46812 (4.0\%)& 0.026 & 0.026\\
%&& 0.1 & 105326 (8.9\%)& 0.0068& 0.0069\\ \hline
%\multirow{3}{*}{C} & \multirow{3}{*}{26}&0.2& 29289 (2.2\%) & 0.084 & 0.085\\
%& & 0.15 & 52070 (3.9\%)& 0.084 & 0.084\\
% &  & 0.1 & 117156 (8.8\%)& 0.080 & 0.082\\ \hline
% \multirow{3}{*}{D} & \multirow{3}{*}{20} & 0.2 & 23375 (2.2\%)& 0.074 & 0.080\\
%&  & 0.15 & 41555 (4.0\%)& 0.034 & 0.037\\
%&  & 0.1 & 93497 (8.9\%)& 0.024 & 0.024\\
%
%\hline
%\end{tabular}
%\label{expone}
%
%\vspace{1em}
%
%\begin{center}
%$n$ = Number of features of system.\\
%$\gamma$ = User specified maximum error.\\
%Samples = Number \& proportion of samples used.\\
%mean = Average actual error from the 10 runs.\\
%max = Maximum actual error from the 10 runs.
%\end{center}
%
%\end{table}



\bibliographystyle{plain}
\balance
\nocite{*}
\bibliography{ref}












\end{document}